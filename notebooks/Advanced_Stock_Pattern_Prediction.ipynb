{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2181eae5-76c4-45c3-8a79-a522981a2280",
   "metadata": {},
   "source": [
    "### Advanced Stock Pattern Prediction using LSTM with the Attention Mechanism in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ab3f9-6fc0-4606-8d3c-71e197f4ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -qqq\n",
    "!pip install keras -qqq\n",
    "!pip install yfinance -qqq\n",
    "!pip install mplfinance -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271a084-55f6-462b-b9ed-04cb5d7f0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8b4d9-8274-427f-9a27-993690606d91",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf933af-fa75-4a60-bfcc-f5316f27a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch AAPL data\n",
    "aapl_data = yf.download('AAPL', start='2020-01-01', end='2024-01-01')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "aapl_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1464c8-0a2a-40ab-8038-2ec2eae87efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "aapl_data.isnull().sum()\n",
    "\n",
    "# Filling missing values, if any\n",
    "aapl_data.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e5059-5afe-4e10-99e9-9c923145ba88",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490a871-ebc3-45b8-beb5-208413cf5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "aapl_data_scaled = scaler.fit_transform(aapl_data['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76820d72-53df-4b04-8748-3556673c679c",
   "metadata": {},
   "source": [
    "### Creating Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82475409-5365-4535-905e-96ca305fed62",
   "metadata": {},
   "source": [
    "LSTM models require input to be in a sequence format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f013b-8847-413b-8ca6-258b4c21e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(60, len(aapl_data_scaled)):\n",
    "    X.append(aapl_data_scaled[i-60:i, 0])\n",
    "    y.append(aapl_data_scaled[i, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4687c0-7984-4cfc-81a2-60a6c0e220fa",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8202bd8-eafe-4f4f-9bae-09ebb161c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Split Ratio: Typically, 80% of data is used for training and 20% for testing.\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1571a-bc52-4040-94c6-c43725622971",
   "metadata": {},
   "source": [
    "### Reshaping Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eaa511-df73-4269-881e-71d8ef46da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff63eaa-4ab8-4be2-bd79-51e3424e5bff",
   "metadata": {},
   "source": [
    "### Building the LSTM with Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577727e-1de7-4dca-983c-9f50ccaab0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LSTM Layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, AdditiveAttention, Permute, Reshape, Multiply\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Adding LSTM layers with return_sequences=True\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963bd73-46ff-459e-9530-d0e8b0dc8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating the Attention Mechanism: Enhance the model’s ability to focus on relevant time steps\n",
    "\n",
    "# Adding self-attention mechanism\n",
    "# The attention mechanism\n",
    "attention = AdditiveAttention(name='attention_weight')\n",
    "# Permute and reshape for compatibility\n",
    "model.add(Permute((2, 1))) \n",
    "model.add(Reshape((-1, X_train.shape[1])))\n",
    "attention_result = attention([model.output, model.output])\n",
    "multiply_layer = Multiply()([model.output, attention_result])\n",
    "# Return to original shape\n",
    "model.add(Permute((2, 1))) \n",
    "model.add(Reshape((-1, 50)))\n",
    "\n",
    "# Adding a Flatten layer before the final Dense layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Final Dense layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850b5dd-0a30-4d9e-aaa5-5956e6e50286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the Model: To enhance the model’s performance and reduce the risk of overfitting\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Adding Dropout and Batch Normalization\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9cfeb0-f407-4f92-870b-fd5da7ca5946",
   "metadata": {},
   "source": [
    "**Dropout** helps in preventing overfitting by randomly setting a fraction of the input units to 0 at each update during training, and **Batch Normalization** stabilizes the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222da509-04e0-4d35-a073-fa8e89d05c6c",
   "metadata": {},
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a1f91-b0ea-4db4-b546-79465bd8a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a470cd9-d5be-41a4-a0ab-c48b253b1516",
   "metadata": {},
   "source": [
    "It’s beneficial to view the summary of the model to understand its structure and number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61f2ae-1f26-451f-9925-7caa1826100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706daa7-a59c-421d-a00c-fc7420c77399",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5de51c-8b2b-4141-90a4-791306767d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are already defined and preprocessed\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f1ea2-e19f-43fb-b31d-3b1686291a3d",
   "metadata": {},
   "source": [
    "### Overfitting and How to Avoid It"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9e860-04dc-4611-b994-e9019a4bbcdc",
   "metadata": {},
   "source": [
    "**Overfitting** occurs when a model learns patterns specific to the training data, which do not generalize to new data. Here are ways to avoid overfitting:\n",
    "\n",
    "- **Validation Set**: Using a validation set (as we did in the training code) helps in monitoring the model’s performance on unseen data.\n",
    "\n",
    "- **Early Stopping**: This technique stops training when the model’s performance on the validation set starts to degrade. Implementing early stopping in Keras is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6ad69-bf32-4bf3-9825-b22d97a35851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26f6b5-f370-4c8a-8d58-f8b52af321d8",
   "metadata": {},
   "source": [
    "**Regularization Techniques**: Techniques like Dropout and Batch Normalization, which are already included in our model, also help in reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccff722-25e2-4537-8902-7818790d88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "\n",
    "# Callback to save the model periodically\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Callback to reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "# Callback for TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Callback to log details to a CSV file\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "\n",
    "# Combining all callbacks\n",
    "callbacks_list = [early_stopping, model_checkpoint, reduce_lr, tensorboard, csv_logger]\n",
    "\n",
    "# Fit the model with the callbacks\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=25, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3471ed-9738-46f4-84ec-1d6a38b5f282",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944cdaf-4eeb-4668-9d96-413906cdf7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_test and y_test to Numpy arrays if they are not already\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Ensure X_test is reshaped similarly to how X_train was reshaped\n",
    "# This depends on how you preprocessed the training data\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Now evaluate the model on the test data\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df44d11-2702-483b-bdb0-b656635a4b7f",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b9418-b688-47bd-bd86-834d150780b3",
   "metadata": {},
   "source": [
    "In addition to the loss, other metrics can provide more insights into the model's performance. For regression tasks like ours, common metrics include:\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: This measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "\n",
    "- **Root Mean Square Error (RMSE)**: This is the square root of the average of squared differences between prediction and actual observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b95244-4e4c-445c-8c19-e34beeb558b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating MAE and RMSE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(\"Mean Absolute Error: \", mae)\n",
    "print(\"Root Mean Square Error: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f0913-35f6-4acb-9d58-32982716f648",
   "metadata": {},
   "source": [
    "Both **MAE** and **RMSE** are measures of prediction accuracy for a regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3555b5-9e3f-4f00-a5ed-61511ede8d36",
   "metadata": {},
   "source": [
    "**MAE** measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df09866-d5fb-43b7-8c03-c168d033caf6",
   "metadata": {},
   "source": [
    "**RMSE** is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation. The RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bbd92-a914-4746-b2ed-1162e3dcd9ab",
   "metadata": {},
   "source": [
    "### Predicting the Next 4 Candles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee0a8d-bf9f-4f08-9447-c9db9abb4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fetching the latest 60 days of AAPL stock data\n",
    "data = yf.download('AAPL', period='60d', interval='1d')\n",
    "\n",
    "# Selecting the 'Close' price and converting to numpy array\n",
    "closing_prices = data['Close'].values\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(closing_prices.reshape(-1,1))\n",
    "\n",
    "# Since we need the last 60 days to predict the next day, we reshape the data accordingly\n",
    "X_latest = np.array([scaled_data[-60:].reshape(60)])\n",
    "\n",
    "# Reshaping the data for the model (adding batch dimension)\n",
    "X_latest = np.reshape(X_latest, (X_latest.shape[0], X_latest.shape[1], 1))\n",
    "\n",
    "# Making predictions for the next 4 candles\n",
    "predicted_stock_price = model.predict(X_latest)\n",
    "predicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n",
    "\n",
    "print(\"Predicted Stock Prices for the next 4 days: \", predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aba08d-b035-46c2-9a9a-539945743041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fetch the latest 60 days of AAPL stock data\n",
    "data = yf.download('AAPL', period='60d', interval='1d')\n",
    "\n",
    "# Select 'Close' price and scale it\n",
    "closing_prices = data['Close'].values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(closing_prices)\n",
    "\n",
    "# Predict the next 4 days iteratively\n",
    "predicted_prices = []\n",
    "current_batch = scaled_data[-60:].reshape(1, 60, 1)  # Most recent 60 days\n",
    "\n",
    "for i in range(4):  # Predicting 4 days\n",
    "    # Get the prediction (next day)\n",
    "    next_prediction = model.predict(current_batch)\n",
    "    \n",
    "    # Reshape the prediction to fit the batch dimension\n",
    "    next_prediction_reshaped = next_prediction.reshape(1, 1, 1)\n",
    "    \n",
    "    # Append the prediction to the batch used for predicting\n",
    "    current_batch = np.append(current_batch[:, 1:, :], next_prediction_reshaped, axis=1)\n",
    "    \n",
    "    # Inverse transform the prediction to the original price scale\n",
    "    predicted_prices.append(scaler.inverse_transform(next_prediction)[0, 0])\n",
    "\n",
    "print(\"Predicted Stock Prices for the next 4 days: \", predicted_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22173302-73f8-4e73-bede-6f549a43b713",
   "metadata": {},
   "source": [
    "### Visualization of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a902909-3328-4095-bc7e-f96f4e9549d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mpl_dates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data' is your DataFrame with the fetched AAPL stock data\n",
    "# Make sure it contains Open, High, Low, Close, and Volume columns\n",
    "\n",
    "# Creating a list of dates for the predictions\n",
    "last_date = data.index[-1]\n",
    "next_day = last_date + pd.Timedelta(days=1)\n",
    "prediction_dates = pd.date_range(start=next_day, periods=4)\n",
    "\n",
    "# Assuming 'predicted_prices' is your list of predicted prices for the next 4 days\n",
    "predictions_df = pd.DataFrame(index=prediction_dates, data=predicted_prices, columns=['Close'])\n",
    "\n",
    "# Plotting the actual data with mplfinance\n",
    "mpf.plot(data, type='candle', style='charles', volume=True)\n",
    "\n",
    "# Overlaying the predicted data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(predictions_df.index, predictions_df['Close'], linestyle='dashed', marker='o', color='red')\n",
    "\n",
    "plt.title(\"AAPL Stock Price with Predicted Next 4 Days\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dffa54-f2a8-4d41-ace8-fb42547f019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mplfinance as mpf\n",
    "import matplotlib.dates as mpl_dates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fetch the latest 60 days of AAPL stock data\n",
    "data = yf.download('AAPL', period='64d', interval='1d') # Fetch 64 days to display last 60 days in the chart\n",
    "\n",
    "# Select 'Close' price and scale it\n",
    "closing_prices = data['Close'].values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(closing_prices)\n",
    "\n",
    "# Predict the next 4 days iteratively\n",
    "predicted_prices = []\n",
    "current_batch = scaled_data[-60:].reshape(1, 60, 1)  # Most recent 60 days\n",
    "\n",
    "for i in range(4):  # Predicting 4 days\n",
    "    next_prediction = model.predict(current_batch)\n",
    "    next_prediction_reshaped = next_prediction.reshape(1, 1, 1)\n",
    "    current_batch = np.append(current_batch[:, 1:, :], next_prediction_reshaped, axis=1)\n",
    "    predicted_prices.append(scaler.inverse_transform(next_prediction)[0, 0])\n",
    "\n",
    "# Creating a list of dates for the predictions\n",
    "last_date = data.index[-1]\n",
    "next_day = last_date + pd.Timedelta(days=1)\n",
    "prediction_dates = pd.date_range(start=next_day, periods=4)\n",
    "\n",
    "# Adding predictions to the DataFrame\n",
    "predicted_data = pd.DataFrame(index=prediction_dates, data=predicted_prices, columns=['Close'])\n",
    "\n",
    "# Combining both actual and predicted data\n",
    "combined_data = pd.concat([data['Close'], predicted_data['Close']])\n",
    "combined_data = combined_data[-64:] # Last 60 days of actual data + 4 days of predictions\n",
    "\n",
    "# Plotting the actual data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(data.index[-60:], data['Close'][-60:], linestyle='-', marker='o', color='blue', label='Actual Data')\n",
    "\n",
    "# Plotting the predicted data\n",
    "plt.plot(prediction_dates, predicted_prices, linestyle='-', marker='o', color='red', label='Predicted Data')\n",
    "\n",
    "plt.title(\"AAPL Stock Price: Last 60 Days and Next 4 Days Predicted\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4f0ad-5d60-4784-8eb4-86234d30de5d",
   "metadata": {},
   "source": [
    "Key points include:\n",
    "\n",
    "- LSTM’s ability to capture long-term dependencies in time-series data.\n",
    "- The added advantage of the attention mechanism in focusing on relevant data points.\n",
    "- The detailed process of building, training, and evaluating the LSTM model.\n",
    "\n",
    "\n",
    "While LSTM models with attention are powerful, they have limitations:\n",
    "\n",
    "- The assumption that historical patterns will repeat in similar ways can be problematic, especially in volatile markets.\n",
    "- External factors like market news and global events, not captured in historical price data, can significantly influence stock prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
